setup: 'eanqg_squad_v1'

#########################################################################################################
# ---------------------------------------- DATA CONFIGURATIONS ---------------------------------------- #
#########################################################################################################

cached_train_path: 'data/squad_split_v1/train.pt'
cached_dev_path: 'data/squad_split_v1/dev.pt'
cached_test_path: 'data/squad_split_v1/test.pt'
cached_vocabularies_path: 'data/squad_split_v1/vocab.pt'

#cached_train_path: 'data/newsqa_v2/train.pt'
#cached_dev_path: 'data/newsqa_v2/dev.pt'
#cached_test_path: 'data/newsqa_v2/test.pt'
#cached_vocabularies_path: 'data/newsqa_v2/vocab.pt'

vocab_size: 45000

min_word_frequency: 1

max_source_length: 200
max_target_length: 50


##########################################################################################################
# ---------------------------------------- MODEL CONFIGURATIONS ---------------------------------------- #
##########################################################################################################

model:
#    Embeddings
    embedding_size: 300
    feature_num: 2      # feature_tags: ['pos', 'ner']
    feature_tag_embedding_size: 16
    answer_tag_embedding_size: 16

#   Encoder
    brnn: True
    enc_num_layers: 2
    enc_rnn_size: 512

#   Context Attention
    ctx_attn_size: 512

#   Decoder
    input_feed: True
    dec_rnn_size: 512
    dec_num_layers: 2

    maxout_pool_size: 2

#    hidden_size: 512
    num_layers: 2
    dropout: 0.5
    layer_norm_eps: 0.000000000001
    hidden_dropout_prob: 0.1

    use_pointer: True

# Initialization
param_init: 0.1
param_init_glorot: True


#########################################################################################################
# -------------------------------------- TRAIN CONFIGURATIONS ----------------------------------------- #
#########################################################################################################

cached_model_dir: checkpoints


train:
    num_train_epochs: 30
    #num_train_steps: 25000
    valid_steps: 500

    batch_size: 64

    optimizer:
    #    name
        optimizer_name: sgd
#        optimizer_name: adam
        learning_rate: 0.1
#        learning_rate: 0.001
        momentum: 0.8
#        momentum: 0.0
        weight_decay: 0.000000      # L2 regularization.
        adam_beta1: 0.9
        adam_beta2: 0.999


    #   Learning rate scheduler: decay
        learning_rate_decay: 0.5
        # empty will be converted to NoneType while 'Null' will be converted to a tuple
        decay_method: Null

        start_decay_steps: 5
        decay_steps: 5

        decay_bad_cnt: 2

        n_warmup_steps: 10000

    #    parameter clip
        max_grad_norm: 5
        max_weight_value: 32

    criterion:

        reduction: 'sum'

        copy: True
        copy_weight: 1.0

        coverage: True
        coverage_weight: 0.4

    #   Checkpoint Manager
    checkpoint:
        max_to_keep: 10

    scorer:
        #   Early stop scheduler:
#        start_stop_steps: 5000
#        stop_steps: 200

        criteria: ['acc']
        tolerance: 10
#        early_stop_tolerance: 8
#        learning_rate_decay_tolerance: 2



#########################################################################################################
# ------------------------------------- INFERENCE CONFIGURATIONS ------------------------------------- #
#########################################################################################################

inference:
    beam_size: 10
    min_decode_step: 6
    max_decode_step: 50

    beta: 0.0

    output: generated_question.txt